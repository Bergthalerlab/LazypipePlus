Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job stats:
job                    count    min threads    max threads
-------------------  -------  -------------  -------------
all                        1              1              1
create_reports             1              8              8
format_hsearch_main        1              1              1
krona_graph                1              1              1
pack                       1              1              1
sans_orfs                  1              8              8
summary_excel              1              1              1
total                      7              1              8


[Mon Aug  1 11:08:01 2022]
rule sans_orfs:
    input: results/sample2/ORFs.aa.fa
    output: results/sample2/dbhits.sans.tsv
    log: logs/sample2/sans_orfs.log
    jobid: 7
    wildcards: sample=sample2
    threads: 8
    resources: tmpdir=/tmp

python3.10 SANSPANZ.3/runsanspanz.py -m SANStopHtaxid --SANS_H 5 -R  -i results/sample2/ORFs.aa.fa -o results/sample2/dbhits.sans.tsv &> logs/sample2/sans_orfs.log
[Mon Aug  1 11:08:01 2022]
Error in rule sans_orfs:
    jobid: 7
    output: results/sample2/dbhits.sans.tsv
    log: logs/sample2/sans_orfs.log (check log file(s) for error message)
    shell:
        python3.10 SANSPANZ.3/runsanspanz.py -m SANStopHtaxid --SANS_H 5 -R  -i results/sample2/ORFs.aa.fa -o results/sample2/dbhits.sans.tsv &> logs/sample2/sans_orfs.log
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/ptriska/Desktop/metagenomics/MPX_experiment/lazypipe/.snakemake/log/2022-08-01T110800.674210.snakemake.log
